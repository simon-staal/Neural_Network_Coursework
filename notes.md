NOTES
=====
This document is to help keep track of the research performed to justify design decisions for our NN.

Activation functions
====================
We are only going to use linear activation functions, with a ReLU as our final activation function
https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8

Optimizers
============
Options:
- Adam
- LBFGS

Justify what kind of optimizer to use:
https://soham.dev/posts/linear-regression-pytorch/
http://sagecal.sourceforge.net/pytorch/index.html
